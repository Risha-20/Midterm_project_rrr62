{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19863597-1360-47dd-af4e-b46a9bb265d8",
   "metadata": {},
   "source": [
    "## Data Mining : MidTerm Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edcd3a3-7d06-4f0f-b390-0e9d999c1ae4",
   "metadata": {},
   "source": [
    "<b>Name:</b> Risha R Dinesh<br>\n",
    "<b>Professor:</b> Yasser Abduallah<br>\n",
    "<b>UCID:</b> rrr62<br>\n",
    "<b>Email_Id:</b> rrr62@njit.edu<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41cc593-8e72-4b69-86ae-9344b4efc878",
   "metadata": {},
   "source": [
    "<b>Topic:</b> A Comparative Study of Brute-Force and Apriori algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd328f-dbec-43d5-87a1-b6feb9f65808",
   "metadata": {},
   "source": [
    "<b>Introduction:</b><br>This project details the implementation of two algorithms: Brute-Force and Apriori for mining frequent itemsets and generating association rules from transactional data. The purpose is to compare the efficiency and effectiveness of each of these algorithms generating frequent itemsets and rules while measuring their execution time under various support and confidence thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f8d18-f45c-4c0a-9d83-edb2ed5dcc3c",
   "metadata": {},
   "source": [
    "<b>Import Statements</b><br>\n",
    "This cell imports the necessary libraries:\n",
    "1. Itertools for generating combinations\n",
    "2. Time for measuring execution time\n",
    "3. Pandas for data manipulation and analysis\n",
    "4. Apriori and association_rules from mixtend.frequent_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c988c0c-97b6-410e-b593-7e13544a74c0",
   "metadata": {},
   "source": [
    "<I>Note: Before executing the cell install pandas and mlxtend libraries.<I>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5304a221-6b41-45c0-8604-bf974d0ecbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\users\\risha\\anaconda3\\lib\\site-packages (0.23.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\risha\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from mlxtend) (1.15.3)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\risha\\appdata\\roaming\\python\\python313\\site-packages (from mlxtend) (2.3.2)\n",
      "Requirement already satisfied: scikit-learn>=1.3.1 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from mlxtend) (1.6.1)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from mlxtend) (3.10.0)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from mlxtend) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from scikit-learn>=1.3.1->mlxtend) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mlxtend pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60683e78-a4ee-4aab-bd88-95778a72b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa5788d-20ce-4825-8741-e65e0d768e67",
   "metadata": {},
   "source": [
    "<b>Calculate Support Function</b><br>\n",
    "This function calculates the support for a given itemset in a set of transactions. Support is the fraction of transactions that contain the itemset. It's a crucial metric in association rule mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325c452d-f3c3-4524-89d4-e7c9d8d12ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate support for an itemset.\n",
    "\n",
    "def calculate_support(transactions, itemset):\n",
    "    return sum(1 for transaction in transactions if set(itemset).issubset(set(transaction))) / len(transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3d2b47-e6e2-40ab-8dd6-4b887e90643c",
   "metadata": {},
   "source": [
    "<b>Brute Force Frequent Itemsets Function</b><br>\n",
    "This function implements a brute force approach to find the frequent itemsets.\n",
    "1. It starts with individual items and progressively increases the itemset size.\n",
    "2. For each size, it generates all possible combinations of items.\n",
    "3. It calculates the support for each combination and keeps those that meet the minimum support threshold.\n",
    "4. The process continues until no new frequent itemsets are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2338b3ce-90a4-488f-bfa2-eb5e3dee6c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute Force algorithm to generate frequent itemset.\n",
    "\n",
    "def brute_force_frequent_itemsets(transactions, min_support):\n",
    "    items = sorted(set(item for transaction in transactions for item in transaction)) # get the unique items from the transactions.\n",
    "    frequent_itemsets = []\n",
    "    itemset_size = 1\n",
    "    \n",
    "    while True:\n",
    "        candidate_itemsets = list(itertools.combinations(items, itemset_size))     # Generate all combinations of itemset from current size.  \n",
    "        current_frequent_itemsets = []\n",
    "        \n",
    "        for itemset in candidate_itemsets:\n",
    "            support = calculate_support(transactions, itemset)\n",
    "            if support >= min_support:\n",
    "                current_frequent_itemsets.append((itemset, support))\n",
    "        \n",
    "        if not current_frequent_itemsets:\n",
    "            break\n",
    "        \n",
    "        frequent_itemsets.extend(current_frequent_itemsets)\n",
    "        itemset_size += 1\n",
    "\n",
    "    return frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37952a02-33e4-4e6a-9a68-8cb268e0b5db",
   "metadata": {},
   "source": [
    "<b>Generate Association Rules Function</b><br>\n",
    "This function generates association rules from the frequent itemsets:\n",
    "1. For each frequent itemset with more than one item, it generates all possible antecedent-consequent pairs.\n",
    "2. It calculates the confidence for each rule.\n",
    "3. Rules that meet the minimum confidence threshold are kept and returns a list of rules with <br>\n",
    "their antecedents, consequents, confidence and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60da343f-b462-4437-96e2-10237720b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_association_rules(frequent_itemsets, transactions, min_confidence):\n",
    "    rules = []\n",
    "    for itemset, itemset_support in frequent_itemsets:\n",
    "        if len(itemset) > 1:\n",
    "            for i in range(1, len(itemset)):\n",
    "                for antecedent in itertools.combinations(itemset, i):\n",
    "                    consequent = tuple(item for item in itemset if item not in antecedent)\n",
    "                    antecedent_support = calculate_support(transactions, antecedent)\n",
    "                    confidence = itemset_support / antecedent_support\n",
    "                    if confidence >= min_confidence:\n",
    "                        rules.append((antecedent, consequent, confidence, itemset_support))\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e247214-fe6c-4729-a7d0-a3da949b2153",
   "metadata": {},
   "source": [
    "<b>Prepare Transaction DataFrame Function</b><br>This function prepares the transaction data for use with the Apriori algorithm from mlxtend:\n",
    "\n",
    "1. It creates a list of all unique items across all transactions.<br>\n",
    "2. It then creates a boolean DataFrame where each row represents a transaction and each column represents an item.<br>\n",
    "3. A True value indicates the presence of an item in a transaction, while False indicates its absence.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e79f4-0173-439e-a513-a8f64f8aebcd",
   "metadata": {},
   "source": [
    "This format is required for the mlxtend implementation of Apriori algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4c3873a-011d-4e3a-95d1-d7f3b4119ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_transaction_df(transactions):\n",
    "    items = sorted(set(item for transaction in transactions for item in transaction))\n",
    "    return pd.DataFrame([[item in transaction for item in items] for transaction in transactions], columns=items).astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9ab6fe-cccb-4c84-881d-e755233089b2",
   "metadata": {},
   "source": [
    "<b>Run All Algorithms Function</b><br>\n",
    "This function runs all the algorithms (Brute Force and Apriori) on the given transactions:\n",
    "1. It first prepares the transaction data in the required format.\n",
    "2. For each algorithm, it:\n",
    "- Measures the execution time\n",
    "- Finds frequent itemsets\n",
    "- Generates association rules\n",
    "- Prints the results\n",
    "3. It handles potential errors in the Apriori algorithm.\n",
    "4. Finally, it returns the execution times for the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d092fcc-0459-4905-a331-424e8c84a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_algorithms(transactions, min_support, min_confidence):\n",
    "    transaction_df = prepare_transaction_df(transactions)\n",
    "\n",
    "    # Brute force algorithm\n",
    "    print(\"\\nRunning Brute Force Algorithm...\")\n",
    "    start_time = time.time()\n",
    "    frequent_itemsets_brute = brute_force_frequent_itemsets(transactions, min_support)\n",
    "    rules_brute = generate_association_rules(frequent_itemsets_brute, transactions, min_confidence)\n",
    "    brute_time = time.time() - start_time\n",
    "    print(f\"Brute Force Time: {brute_time:.4f} seconds\")\n",
    "    print_results(\"Brute Force\", frequent_itemsets_brute, rules_brute)\n",
    "\n",
    "    # Apriori Algorithm\n",
    "    print(\"\\nRunning Apriori Algorithm...\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        frequent_itemsets_apriori = apriori(transaction_df, min_support=min_support, use_colnames=True)\n",
    "        apriori_time = time.time() - start_time\n",
    "        \n",
    "        if frequent_itemsets_apriori.empty:\n",
    "            print(\"Apriori did not find any frequent itemsets.\")\n",
    "        else:\n",
    "            rules_apriori = association_rules(frequent_itemsets_apriori, metric=\"confidence\", min_threshold=min_confidence)\n",
    "            print(f\"Apriori Time: {apriori_time:.4f} seconds\")\n",
    "            print_results(\"Apriori\", frequent_itemsets_apriori, rules_apriori)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Apriori algorithm execution: {e}\")\n",
    "        apriori_time = time.time() - start_time\n",
    "    \n",
    "    return brute_time, apriori_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec1cf2-ba56-4908-b2b0-51d6f36be5e8",
   "metadata": {},
   "source": [
    "<b> Read CSV and Prepare Transactions Function</b><br>\n",
    "This function reads transaction and itemset data from CSV files and prepares it for analysis:\n",
    "1. It reads both the transaction and itemset CSV files.\n",
    "2. It creates a mapping of item numbers to item names if available.\n",
    "3. It processes each transaction, handling both comma-separated strings and individual items.\n",
    "4. It returns a list of transactions, where each transaction is a list of items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f5425-ddfb-4049-871b-734cc5ce521d",
   "metadata": {},
   "source": [
    "<I>Note: Make sure the Datasets are kept in the same directory with the same naming conventions.<I>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e80bc79a-c69e-4f11-947a-149a21f04984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_and_prepare_transactions(transaction_file, itemset_file):\n",
    "    try:\n",
    "        df_trans = pd.read_csv(transaction_file)\n",
    "        df_items = pd.read_csv(itemset_file)\n",
    "        \n",
    "        if 'Item #' in df_items.columns and 'Item Name' in df_items.columns:\n",
    "            item_map = dict(zip(df_items['Item #'], df_items['Item Name']))\n",
    "        else:\n",
    "            item_map = None\n",
    "        \n",
    "        transactions = []\n",
    "        for _, row in df_trans.iterrows():\n",
    "            transaction = []\n",
    "            for item in row:\n",
    "                if isinstance(item, str):\n",
    "                    items = [i.strip() for i in item.split(',') if i.strip()]\n",
    "                    transaction.extend(items)\n",
    "                elif pd.notna(item):\n",
    "                    transaction.append(str(item))\n",
    "            if transaction:\n",
    "                transactions.append(transaction)\n",
    "        \n",
    "        return transactions\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the CSV files: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62872536-55f1-49bb-8105-68ebe0fff30d",
   "metadata": {},
   "source": [
    "<b>Print Results Function</b><br>\n",
    "This function prints the results of the algorithms in a readable format:\n",
    "1. It first prints the frequent itemsets with their support values.\n",
    "2. Then it prints the association rules, showing the antecedent, consequent, confidence, and support for each rule.\n",
    "3. It handles both DataFrame and list formats, accommodating the different output formats of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "836047c9-b368-4d58-9faa-d0a71e9f5e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(algorithm_name, frequent_itemsets, rules):\n",
    "    print(f\"\\n{algorithm_name} Results:\")\n",
    "    print(\"Frequent Itemsets:\")\n",
    "    if isinstance(frequent_itemsets, pd.DataFrame):\n",
    "        for _, row in frequent_itemsets.iterrows():\n",
    "            print(f\"Items: {set(row['itemsets'])}, Support: {row['support']*100:.2f}%\")\n",
    "    else:\n",
    "        for itemset, support in frequent_itemsets:\n",
    "            print(f\"Items: {set(itemset)}, Support: {support*100:.2f}%\")\n",
    "    \n",
    "    print(\"\\nAssociation Rules:\")\n",
    "    if isinstance(rules, pd.DataFrame):\n",
    "        for _, rule in rules.iterrows():\n",
    "            print(f\"Rule: {set(rule['antecedents'])} -> {set(rule['consequents'])}\")\n",
    "            print(f\"Confidence: {rule['confidence']*100:.2f}%, Support: {rule['support']*100:.2f}%\")\n",
    "            print()\n",
    "    else:\n",
    "        for antecedent, consequent, confidence, support in rules:\n",
    "            print(f\"Rule: {set(antecedent)} -> {set(consequent)}\")\n",
    "            print(f\"Confidence: {confidence*100:.2f}%, Support: {support*100:.2f}%\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cdabb5-2f67-462f-94b1-ce9e832e59bc",
   "metadata": {},
   "source": [
    "<b>Main Function</b><br>\n",
    "This is the main function that orchestrates the entire process:\n",
    "1. It defines a dictionary of available stores and their corresponding CSV files.\n",
    "2. It prompts the user to select a store for analysis.\n",
    "3. It reads and prepares the transaction data for the selected store.\n",
    "4. It prompts the user for minimum support and confidence thresholds.\n",
    "5. It runs all three algorithms on the data and measures their execution times.\n",
    "6. Finally, it prints the execution times and identifies the fastest algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a30f4d-be72-4912-8c20-28644ed25e4f",
   "metadata": {},
   "source": [
    "<b>Tutorial Note:</b> The General Store is small dataset so if you want to try low support values, try on that dataset otherwise the brute force takes time to execute but it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f6a42b7-b2e2-4cae-ba1f-8579ce78b570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available stores:\n",
      "1. Amazon\n",
      "2. Bestbuy\n",
      "3. General\n",
      "4. K-mart\n",
      "5. Nike\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of the store you want to analyze:  4\n",
      "Enter the minimum support (as a percentage between 0 and 100):  50\n",
      "Enter the minimum confidence (as a percentage between 0 and 100):  70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Brute Force Algorithm...\n",
      "Brute Force Time: 0.0514 seconds\n",
      "\n",
      "Brute Force Results:\n",
      "Frequent Itemsets:\n",
      "Items: {'Bed Skirts'}, Support: 55.00%\n",
      "Items: {'Decorative Pillows'}, Support: 50.00%\n",
      "Items: {'Kids Bedding'}, Support: 60.00%\n",
      "Items: {'Shams'}, Support: 55.00%\n",
      "Items: {'Sheets'}, Support: 50.00%\n",
      "Items: {'Bed Skirts', 'Kids Bedding'}, Support: 50.00%\n",
      "Items: {'Kids Bedding', 'Sheets'}, Support: 50.00%\n",
      "\n",
      "Association Rules:\n",
      "Rule: {'Bed Skirts'} -> {'Kids Bedding'}\n",
      "Confidence: 90.91%, Support: 50.00%\n",
      "\n",
      "Rule: {'Kids Bedding'} -> {'Bed Skirts'}\n",
      "Confidence: 83.33%, Support: 50.00%\n",
      "\n",
      "Rule: {'Kids Bedding'} -> {'Sheets'}\n",
      "Confidence: 83.33%, Support: 50.00%\n",
      "\n",
      "Rule: {'Sheets'} -> {'Kids Bedding'}\n",
      "Confidence: 100.00%, Support: 50.00%\n",
      "\n",
      "\n",
      "Running Apriori Algorithm...\n",
      "Apriori Time: 0.0058 seconds\n",
      "\n",
      "Apriori Results:\n",
      "Frequent Itemsets:\n",
      "Items: {'Bed Skirts'}, Support: 55.00%\n",
      "Items: {'Decorative Pillows'}, Support: 50.00%\n",
      "Items: {'Kids Bedding'}, Support: 60.00%\n",
      "Items: {'Shams'}, Support: 55.00%\n",
      "Items: {'Sheets'}, Support: 50.00%\n",
      "Items: {'Bed Skirts', 'Kids Bedding'}, Support: 50.00%\n",
      "Items: {'Kids Bedding', 'Sheets'}, Support: 50.00%\n",
      "\n",
      "Association Rules:\n",
      "Rule: {'Bed Skirts'} -> {'Kids Bedding'}\n",
      "Confidence: 90.91%, Support: 50.00%\n",
      "\n",
      "Rule: {'Kids Bedding'} -> {'Bed Skirts'}\n",
      "Confidence: 83.33%, Support: 50.00%\n",
      "\n",
      "Rule: {'Kids Bedding'} -> {'Sheets'}\n",
      "Confidence: 83.33%, Support: 50.00%\n",
      "\n",
      "Rule: {'Sheets'} -> {'Kids Bedding'}\n",
      "Confidence: 100.00%, Support: 50.00%\n",
      "\n",
      "\n",
      "Execution Times:\n",
      "Brute Force: 0.0514 seconds\n",
      "Apriori: 0.0058 seconds\n",
      "\n",
      "The fastest algorithm is: Apriori\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Define available stores and their corresponding files\n",
    "    stores = {\n",
    "        \"1\": (\"Amazon\", \"Amazon_Transaction.csv\", \"Amazon_Itemset.csv\"),\n",
    "        \"2\": (\"Bestbuy\", \"Bestbuy_Transaction.csv\", \"Bestbuy_Itemset.csv\"),\n",
    "        \"3\": (\"General\", \"General_Transaction.csv\", \"General_Itemset.csv\"),\n",
    "        \"4\": (\"K-mart\", \"K_mart_Transaction.csv\", \"K_mart_Itemset.csv\"),\n",
    "        \"5\": (\"Nike\", \"Nike_Transaction.csv\", \"Nike_Itemset.csv\")\n",
    "    }\n",
    "    \n",
    "    print(\"\\nAvailable stores:\")\n",
    "    for key, (store_name, _, _) in stores.items():\n",
    "        print(f\"{key}. {store_name}\")\n",
    "\n",
    "    # Get user input for store selection.\n",
    "    while True:\n",
    "        choice = input(\"Enter the number of the store you want to analyze: \")\n",
    "        if choice in stores:\n",
    "            store_name, transaction_file, itemset_file = stores[choice]\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter a number between 1 and 5.\")\n",
    "\n",
    "    # Read and prepare transaction data.\n",
    "    try:\n",
    "        transactions = read_csv_and_prepare_transactions(transaction_file, itemset_file)\n",
    "        if not transactions:\n",
    "            print(\"No valid transactions found. Please check your CSV files.\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the CSV files: {e}\")\n",
    "        print(\"Please ensure that the CSV files are properly formatted.\")\n",
    "        return\n",
    "\n",
    "# Get user input for minimum support and minimum confidence.\n",
    "    while True:\n",
    "        try:\n",
    "            min_support = float(input(\"Enter the minimum support (as a percentage between 0 and 100): \"))\n",
    "            if 0 <= min_support <= 100:\n",
    "                min_support /= 100  # Convert to decimal\n",
    "                break\n",
    "            else:\n",
    "                print(\"Please enter a value between 0 and 100.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            min_confidence = float(input(\"Enter the minimum confidence (as a percentage between 0 and 100): \"))\n",
    "            if 0 <= min_confidence <= 100:\n",
    "                min_confidence /= 100  # Convert to decimal\n",
    "                break\n",
    "            else:\n",
    "                print(\"Please enter a value between 0 and 100.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "    brute_time, apriori_time = run_all_algorithms(transactions, min_support, min_confidence)\n",
    "\n",
    "    print(f\"\\nExecution Times:\")\n",
    "    print(f\"Brute Force: {brute_time:.4f} seconds\")\n",
    "    print(f\"Apriori: {apriori_time:.4f} seconds\")\n",
    "\n",
    "    # Print the fastest algorithm.\n",
    "    fastest_algorithm = min((brute_time, 'Brute Force'), (apriori_time, 'Apriori'))[1]\n",
    "    print(f\"\\nThe fastest algorithm is: {fastest_algorithm}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "295bfaed-fb0a-4ad0-adb6-5c530ea6c3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "139d967d-5299-460a-b304-5a43f3d26948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101797c4-44fa-4bfb-bfac-4eaa88f8019d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92505b1-a69d-44c6-bebd-32fdc1757022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
